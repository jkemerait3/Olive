# How to run locally (reproducibility)
1. Install ollama from https://ollama.com/
2. In CLI, run "ollama pull mistral" and "ollama serve" // Alternatively, pull your model of choice and change the value of MODEL_NAME to its name on line 4 of Olive/llm/local_llm.py
3. Navigate to the project directory and run the project in CLI with "python main.py"


